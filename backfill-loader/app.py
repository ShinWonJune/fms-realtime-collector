import os
import logging
import argparse
import sys
from datetime import datetime, timedelta
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from kafka import KafkaConsumer, TopicPartition
from minio import Minio
from minio.error import S3Error
import json
from io import BytesIO
from collections import defaultdict
import signal
import time

'''

ì‚¬ìš© ëª…ë ¹ì–´
docker-compose run --rm backfill-loader python app.py --mode incremental
***incremental ì£¼ì˜ì‚¬í•­***
4ì¼ì´ ì§€ë‚˜ë©´ ì˜¤ë˜ëœ ë°ì´í„° ë¶€í„° ëˆ„ë½ë°œìƒ. offsetì´ ë’¤ë¡œ ë°€ë ¤ì„œ ê·¸ëŸ°ë“¯.
3ì¼ì— í•œë²ˆì”© ì‹¤í–‰ ê¶Œì¥

docker-compose run --rm backfill-loader python app.py --mode scheduler
docker-compose run --rm backfill-loader python app.py --mode continue --from-date 2023-10-01
docker-compose run --rm backfill-loader python app.py --mode fresh

- incremental ëª¨ë“œì—ì„œëŠ” ë§ˆì§€ë§‰ ì»¤ë°‹ëœ ì˜¤í”„ì…‹ë¶€í„° ì‹œì‘
- scheduler ëª¨ë“œì—ì„œëŠ” 3ì¼ì— í•œë²ˆì”© ìì •ì— incremental mode ì‹¤í–‰
- continue ëª¨ë“œì—ì„œëŠ” from_dateê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ë‚ ì§œë¶€í„° ì‹œì‘, ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ë§ˆì§€ë§‰ ì»¤ë°‹ëœ ì˜¤í”„ì…‹ë¶€í„° ì‹œì‘
- fresh ëª¨ë“œì—ì„œëŠ” í•­ìƒ ì²˜ìŒë¶€í„° ì‹œì‘


parquet columns:
objId,rsctypeId,TEMPERATURE1,TEMPERATURE,HUMIDITY1,HUMIDITY,timestamp
timestampëŠ” KTCë¡œ "2025-06-26T12:22:04.499000" í˜•íƒœ


docker-compose build backfill-loader

'''
# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BackfillLoader:
    def __init__(self, mode='fresh', from_date=None, to_date=None, target_date=None):
        # ì‹¤í–‰ ëª¨ë“œ ì„¤ì •
        self.mode = mode
        self.from_date = from_date
        self.to_date = to_date
        self.target_date = target_date
        
        self.date_filter = self._setup_date_filter()

        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ì½ê¸°
        self.kafka_brokers = os.getenv('KAFKA_BROKERS', 'localhost:9092')
        self.kafka_topic = os.getenv('KAFKA_TOPIC', 'fms-temphum')
        self.kafka_group_id = os.getenv('KAFKA_GROUP_ID', 'backfill-loader')
        
        self.minio_endpoint = os.getenv('MINIO_ENDPOINT', 'localhost:9000')
        self.minio_access_key = os.getenv('MINIO_ACCESS_KEY', 'minioadmin')
        self.minio_secret_key = os.getenv('MINIO_SECRET_KEY', 'minioadmin')
        self.minio_bucket = os.getenv('MINIO_BUCKET', 'fms-temphum')
        self.minio_secure = os.getenv('MINIO_SECURE', 'false').lower() == 'true'
        
        logger.info(f"=== BackfillLoader initialized ===")
        logger.info(f"Mode: {self.mode}")
        if self.date_filter:
            logger.info(f"Date filter: {self.date_filter}")
        logger.info(f"From date: {self.from_date}")
        logger.info(f"Kafka Topic: {self.kafka_topic}")
        logger.info(f"Group ID: {self.kafka_group_id}")
        
        # MinIO í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.minio_client = self._init_minio_client()
        
        # Kafka ì»¨ìŠˆë¨¸ ì´ˆê¸°í™”
        self.consumer = self._init_kafka_consumer()
        
        logger.info("BackfillLoader initialized successfully")

    def _setup_date_filter(self):
        """ë‚ ì§œ í•„í„° ì„¤ì • ë° ê²€ì¦"""
        date_filter = {}
        
        try:
            # target_dateê°€ ì§€ì •ëœ ê²½ìš° (ë‹¨ì¼ ë‚ ì§œ)
            if self.target_date:
                target_dt = datetime.strptime(self.target_date, '%Y-%m-%d')
                date_filter['from_datetime'] = target_dt.replace(hour=0, minute=0, second=0)
                date_filter['to_datetime'] = target_dt.replace(hour=23, minute=59, second=59)
                date_filter['description'] = f"Single date: {self.target_date}"
                return date_filter
            
            # from_date ì²˜ë¦¬
            if self.from_date:
                from_dt = datetime.strptime(self.from_date, '%Y-%m-%d')
                date_filter['from_datetime'] = from_dt.replace(hour=0, minute=0, second=0)
            
            # to_date ì²˜ë¦¬
            if self.to_date:
                to_dt = datetime.strptime(self.to_date, '%Y-%m-%d')
                date_filter['to_datetime'] = to_dt.replace(hour=23, minute=59, second=59)
            
            # ë‚ ì§œ ë²”ìœ„ ê²€ì¦
            if 'from_datetime' in date_filter and 'to_datetime' in date_filter:
                if date_filter['from_datetime'] > date_filter['to_datetime']:
                    raise ValueError("from_date cannot be later than to_date")
            
            # ì„¤ëª… ìƒì„±
            if 'from_datetime' in date_filter or 'to_datetime' in date_filter:
                desc_parts = []
                if 'from_datetime' in date_filter:
                    desc_parts.append(f"from {date_filter['from_datetime'].strftime('%Y-%m-%d')}")
                if 'to_datetime' in date_filter:
                    desc_parts.append(f"to {date_filter['to_datetime'].strftime('%Y-%m-%d')}")
                date_filter['description'] = "Date range: " + " ".join(desc_parts)
            
            return date_filter if date_filter else None
            
        except ValueError as e:
            logger.error(f"Invalid date format: {e}")
            raise

    def is_message_in_date_range(self, message_data):
        """ë©”ì‹œì§€ê°€ ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸"""
        if not self.date_filter:
            return True  # ë‚ ì§œ í•„í„°ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë©”ì‹œì§€ í—ˆìš©
        
        try:
            # ë©”ì‹œì§€ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            timestamp_value = None
            if '@timestamp' in message_data:
                timestamp_value = message_data['@timestamp']
            elif 'timestamp' in message_data:
                timestamp_value = message_data['timestamp']
            
            if not timestamp_value:
                return True  # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ í—ˆìš© (ê¸°ë³¸ê°’)
            
            # UTC â†’ KST ë³€í™˜
            msg_kst_datetime = self.convert_utc_to_kst(timestamp_value)
            if not msg_kst_datetime:
                return True  # ë³€í™˜ ì‹¤íŒ¨ì‹œ í—ˆìš©
            
            # ë‚ ì§œ ë²”ìœ„ ì²´í¬
            if 'from_datetime' in self.date_filter:
                if msg_kst_datetime < self.date_filter['from_datetime']:
                    return False
            
            if 'to_datetime' in self.date_filter:
                if msg_kst_datetime > self.date_filter['to_datetime']:
                    return False
            
            return True
            
        except Exception as e:
            logger.warning(f"Error checking date range for message: {e}")
            return True  # ì—ëŸ¬ ë°œìƒì‹œ í—ˆìš©
        
    def _init_minio_client(self):
        """MinIO í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            client = Minio(
                self.minio_endpoint,
                access_key=self.minio_access_key,
                secret_key=self.minio_secret_key,
                secure=self.minio_secure
            )
            
            # ë²„í‚· ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            if not client.bucket_exists(self.minio_bucket):
                client.make_bucket(self.minio_bucket)
                logger.info(f"Created bucket: {self.minio_bucket}")
            
            return client
        except Exception as e:
            logger.error(f"Failed to initialize MinIO client: {e}")
            raise

    def _log_partition_status(self, consumer, partitions):
        """ê° íŒŒí‹°ì…˜ì˜ ì˜¤í”„ì…‹ ìƒíƒœ ë¡œê¹…"""
        logger.info("=== Partition Status ===")
        
        # ê° íŒŒí‹°ì…˜ì˜ ì‹œì‘/ë/í˜„ì¬ ì˜¤í”„ì…‹ í™•ì¸
        beginning_offsets = consumer.beginning_offsets(partitions)
        end_offsets = consumer.end_offsets(partitions)
        
        total_available = 0
        total_remaining = 0
        
        for partition in sorted(partitions, key=lambda p: p.partition):
            current_offset = consumer.position(partition)
            beginning_offset = beginning_offsets.get(partition, 0)
            end_offset = end_offsets.get(partition, 0)
            
            available_messages = end_offset - beginning_offset
            remaining_messages = end_offset - current_offset
            
            total_available += available_messages
            total_remaining += remaining_messages
            
            logger.info(f"Partition {partition.partition:2d}: "
                    f"begin={beginning_offset:8d}, "
                    f"current={current_offset:8d}, "
                    f"end={end_offset:8d}, "
                    f"available={available_messages:8d}, "
                    f"remaining={remaining_messages:8d}")
        
        logger.info(f"TOTAL: available={total_available:8d}, remaining={total_remaining:8d}")
        logger.info("========================")
        
    def _init_kafka_consumer(self):
        """Kafka ì»¨ìŠˆë¨¸ ì´ˆê¸°í™” - ëª¨ë“  íŒŒí‹°ì…˜ ëª…ì‹œì  í• ë‹¹"""
        try:
            logger.info(f"Initializing Kafka consumer for mode: {self.mode}")
            
            # ëª¨ë“œë³„ ì„¤ì •
            if self.mode == 'fresh':
                auto_offset_reset = 'earliest'
                enable_auto_commit = False
                logger.info("Fresh mode: Reading from beginning, not saving offsets")
                
            elif self.mode in ['incremental', 'continue']:
                auto_offset_reset = 'earliest'
                enable_auto_commit = True
                logger.info(f"{self.mode} mode: Reading from last committed offset")
            else:
                raise ValueError(f"Invalid mode: {self.mode}")
            
            consumer = KafkaConsumer(
                bootstrap_servers=self.kafka_brokers.split(','),
                group_id=self.kafka_group_id,
                auto_offset_reset=auto_offset_reset,
                enable_auto_commit=enable_auto_commit,
                auto_commit_interval_ms=5000,
                value_deserializer=lambda x: json.loads(x.decode('utf-8')) if x else None,
                consumer_timeout_ms=30000,
                max_poll_records=1000,
                session_timeout_ms=30000,
                heartbeat_interval_ms=3000,
            )
            
            # ===== í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: ëª¨ë“  íŒŒí‹°ì…˜ ëª…ì‹œì  í• ë‹¹ =====
            
            # 1. í† í”½ì˜ ëª¨ë“  íŒŒí‹°ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            # cluster_metadata = consumer.list_consumer_groups()
            topic_partitions = consumer.partitions_for_topic(self.kafka_topic)
            
            if topic_partitions is None:
                raise Exception(f"Topic '{self.kafka_topic}' not found")
            
            logger.info(f"Found {len(topic_partitions)} partitions for topic '{self.kafka_topic}': {sorted(topic_partitions)}")
            
            # 2. ëª¨ë“  íŒŒí‹°ì…˜ì„ TopicPartition ê°ì²´ë¡œ ìƒì„±
            all_partitions = [TopicPartition(self.kafka_topic, partition) for partition in topic_partitions]
            
            # 3. ëª¨ë“  íŒŒí‹°ì…˜ì„ ëª…ì‹œì ìœ¼ë¡œ í• ë‹¹
            consumer.assign(all_partitions)
            logger.info(f"Explicitly assigned all {len(all_partitions)} partitions")
            
            # 4. ëª¨ë“œë³„ ì˜¤í”„ì…‹ ì„¤ì •
            if self.mode == 'fresh':
                # Fresh ëª¨ë“œ: ëª¨ë“  íŒŒí‹°ì…˜ì„ ì²˜ìŒë¶€í„°
                consumer.seek_to_beginning()
                logger.info("Reset all partitions to beginning for fresh mode")
                
            elif self.mode in ['incremental', 'continue']:
                # Incremental/Continue ëª¨ë“œ: ê° íŒŒí‹°ì…˜ë³„ë¡œ ì»¤ë°‹ëœ ì˜¤í”„ì…‹ í™•ì¸
                for partition in all_partitions:
                    try:
                        # ê° íŒŒí‹°ì…˜ë³„ë¡œ ì»¤ë°‹ëœ ì˜¤í”„ì…‹ í™•ì¸
                        committed_offset = consumer.committed(partition)  # âœ… ì˜¬ë°”ë¥¸ ë©”ì„œë“œ
                        
                        if committed_offset is not None and committed_offset >= 0:
                            # ì»¤ë°‹ëœ ì˜¤í”„ì…‹ì´ ìˆìœ¼ë©´ í•´ë‹¹ ìœ„ì¹˜ë¡œ ì´ë™
                            consumer.seek(partition, committed_offset)
                            logger.info(f"Partition {partition.partition}: seeking to committed offset {committed_offset}")
                        else:
                            # ì»¤ë°‹ëœ ì˜¤í”„ì…‹ì´ ì—†ìœ¼ë©´ ì²˜ìŒë¶€í„° (continue ëª¨ë“œ) ë˜ëŠ” ìµœì‹ ë¶€í„° (incremental ëª¨ë“œ)
                            if self.mode == 'continue':
                                consumer.seek_to_beginning(partition)
                                logger.info(f"Partition {partition.partition}: no committed offset, seeking to beginning")
                            else:  # incremental
                                consumer.seek_to_end(partition)
                                logger.info(f"Partition {partition.partition}: no committed offset, seeking to end")
                    except Exception as e:
                        logger.warning(f"Error getting committed offset for partition {partition.partition}: {e}")
                        # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ë™ì‘
                        if self.mode == 'continue':
                            consumer.seek_to_beginning(partition)
                            logger.info(f"Partition {partition.partition}: error occurred, seeking to beginning")
                        else:  # incremental
                            consumer.seek_to_end(partition)
                            logger.info(f"Partition {partition.partition}: error occurred, seeking to end")
            
            # 5. íŒŒí‹°ì…˜ë³„ í˜„ì¬ ìœ„ì¹˜ í™•ì¸ ë° ë¡œê¹…
            self._log_partition_status(consumer, all_partitions)
            
            return consumer
            
        except Exception as e:
            logger.error(f"Failed to initialize Kafka consumer: {e}")
            raise
    
    def convert_utc_to_kst(self, utc_timestamp):
        """UTC íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ KSTë¡œ ë³€í™˜ (9ì‹œê°„ ì¶”ê°€)"""
        try:
            if isinstance(utc_timestamp, str):
                # ë‹¤ì–‘í•œ ISO í˜•ì‹ ì§€ì›
                if 'Z' in utc_timestamp:
                    utc_dt = datetime.fromisoformat(utc_timestamp.replace('Z', '+00:00'))
                elif '+' in utc_timestamp or utc_timestamp.endswith('00:00'):
                    utc_dt = datetime.fromisoformat(utc_timestamp)
                else:
                    # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°€ì •
                    utc_dt = datetime.fromisoformat(utc_timestamp)
            elif isinstance(utc_timestamp, (int, float)):
                utc_dt = datetime.fromtimestamp(utc_timestamp)
            else:
                utc_dt = utc_timestamp
            
            # UTCì—ì„œ KSTë¡œ ë³€í™˜ (9ì‹œê°„ ì¶”ê°€)
            if utc_dt.tzinfo is None:
                kst_dt = utc_dt + timedelta(hours=9)
            else:
                # íƒ€ì„ì¡´ì´ ìˆìœ¼ë©´ UTCë¡œ ë³€í™˜ í›„ 9ì‹œê°„ ì¶”ê°€
                kst_dt = utc_dt.astimezone().replace(tzinfo=None) + timedelta(hours=9)
            
            return kst_dt
        except Exception as e:
            logger.error(f"Error converting timestamp {utc_timestamp}: {e}")
            return None
    
    def process_data_timestamps(self, all_data):
        """ë°ì´í„°ì˜ @timestampë¥¼ KST timestampë¡œ ë³€í™˜í•˜ê³  ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
        processed_data = []
        timestamps = []
        
        logger.info(f"Processing timestamps for {len(all_data)} records...")
        
        for i, data in enumerate(all_data):
            try:
                # ì›ë³¸ ë°ì´í„° ë³µì‚¬
                processed_record = data.copy()
                
                # @timestamp ì°¾ê¸° ë° ë³€í™˜
                timestamp_value = None
                if '@timestamp' in data:
                    timestamp_value = data['@timestamp']
                elif 'timestamp' in data:
                    timestamp_value = data['timestamp']
                
                if timestamp_value:
                    # UTC â†’ KST ë³€í™˜
                    kst_timestamp = self.convert_utc_to_kst(timestamp_value)
                    
                    if kst_timestamp:
                        # timestamp í•„ë“œë¡œ ì €ì¥ (KST)
                        processed_record['timestamp'] = kst_timestamp.isoformat()
                        timestamps.append(kst_timestamp)
                        
                        # @timestamp ì œê±° (ì¤‘ë³µ ë°©ì§€)
                        if '@timestamp' in processed_record:
                            del processed_record['@timestamp']
                    else:
                        logger.warning(f"Failed to convert timestamp for record {i}")
                        continue
                else:
                    logger.warning(f"No timestamp found in record {i}")
                    continue
                
                processed_data.append(processed_record)
                
                # ì§„í–‰ ìƒí™© ë¡œê·¸ (10000ê°œë§ˆë‹¤)
                if (i + 1) % 10000 == 0:
                    logger.info(f"Processed {i + 1}/{len(all_data)} records...")
                    
            except Exception as e:
                logger.warning(f"Error processing record {i}: {e}")
                continue
        
        if not timestamps:
            logger.error("No valid timestamps found in data")
            return processed_data, None, None
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        min_timestamp = min(timestamps)
        max_timestamp = max(timestamps)
        
        logger.info(f"Successfully processed {len(processed_data)} records")
        logger.info(f"Date range: {min_timestamp.strftime('%Y-%m-%d %H:%M:%S')} ~ {max_timestamp.strftime('%Y-%m-%d %H:%M:%S')} (KST)")
        
        return processed_data, min_timestamp, max_timestamp
    
    def generate_filename(self, min_timestamp, max_timestamp):
        """ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±"""
        if not min_timestamp or not max_timestamp:
            # ê¸°ë³¸ íŒŒì¼ëª… (í˜„ì¬ ë‚ ì§œ)
            today = datetime.now().strftime('%Y%m%d')
            return f"{today}_{today}.parquet"
        
        start_date = min_timestamp.strftime('%Y%m%d')
        end_date = max_timestamp.strftime('%Y%m%d')
        
        return f"{start_date}_{end_date}.parquet"
    
    def save_parquet_to_minio(self, df, object_name):
        """DataFrameì„ Parquetìœ¼ë¡œ MinIOì— ì €ì¥"""
        try:
            if df.empty:
                logger.warning(f"Empty DataFrame for {object_name}, skipping save")
                return False
                
            # timestampë¡œ ì •ë ¬
            if 'timestamp' in df.columns:
                df = df.sort_values('timestamp').reset_index(drop=True)
            
            table = pa.Table.from_pandas(df)
            parquet_buffer = BytesIO()
            pq.write_table(table, parquet_buffer)
            parquet_buffer.seek(0)
            
            self.minio_client.put_object(
                self.minio_bucket,
                object_name,
                parquet_buffer,
                length=len(parquet_buffer.getvalue()),
                content_type='application/octet-stream'
            )
            
            logger.info(f"Successfully saved {object_name} to MinIO ({len(df)} records, {len(parquet_buffer.getvalue())} bytes)")
            return True
        except Exception as e:
            logger.error(f"Error saving {object_name}: {e}")
            return False
    
    def collect_all_data(self):
        """Kafkaì—ì„œ ë°ì´í„° ìˆ˜ì§‘ - ëª¨ë“  íŒŒí‹°ì…˜ì—ì„œ ìˆ˜ì§‘"""
        logger.info(f"Starting data collection in {self.mode} mode")
        if self.date_filter:
            logger.info(f"Applying date filter: {self.date_filter['description']}")
        all_data = []
        message_count = 0
        filtered_count = 0
        start_time = datetime.now()
        partition_stats = defaultdict(int)  # íŒŒí‹°ì…˜ë³„ ë©”ì‹œì§€ ìˆ˜ í†µê³„
        
        try:
            # ì´ˆê¸° íŒŒí‹°ì…˜ ìƒíƒœ ì²´í¬
            assigned_partitions = self.consumer.assignment()
            logger.info(f"Starting collection from {len(assigned_partitions)} partitions")
            
            for message in self.consumer:
                if message.value is None:
                    continue
                
                message_count += 1
                
                # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ ì ìš©
                if not self.is_message_in_date_range(message.value):
                    filtered_count += 1
                    
                    # ì„±ëŠ¥ì„ ìœ„í•´ í•„í„°ë§ í†µê³„ë§Œ ê¸°ë¡í•˜ê³  continue
                    if filtered_count % 50000 == 0:
                        logger.info(f"Filtered out {filtered_count} messages so far...")
                    continue
                
                # íŒŒí‹°ì…˜ë³„ í†µê³„ (ì‹¤ì œ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ë§Œ)
                partition_stats[message.partition] += 1
                all_data.append(message.value)
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                if len(all_data) % 10000 == 0:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    logger.info(f"Collected {len(all_data)} messages "
                               f"(processed {message_count}, filtered {filtered_count}) "
                               f"in {elapsed:.1f}s (mode: {self.mode})")
                    
                    # íŒŒí‹°ì…˜ë³„ í†µê³„ ì¶œë ¥
                    if partition_stats:
                        stats_str = ", ".join([f"P{p}:{c}" for p, c in sorted(partition_stats.items())])
                        logger.info(f"Partition stats: {stats_str}")
                
                # # incremental ëª¨ë“œì—ì„œëŠ” ì œí•œëœ ìˆ˜ë§Œ ìˆ˜ì§‘
                # if self.mode == 'incremental' and len(all_data) >= 100000:
                #     logger.info(f"Incremental mode: Collected {len(all_data)} valid messages, stopping")
                #     break
                    
        except Exception as e:
            if "timeout" in str(e).lower():
                logger.info(f"Consumer timeout reached. "
                           f"Processed {message_count} messages, "
                           f"collected {len(all_data)} valid messages, "
                           f"filtered {filtered_count} messages")
            else:
                logger.error(f"Error during data collection: {e}")
                raise
        
        collection_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"Finished data collection in {collection_time:.1f}s")
        logger.info(f"Total processed: {message_count}")
        logger.info(f"Valid messages: {len(all_data)}")
        logger.info(f"Filtered out: {filtered_count}")
        
        if self.date_filter:
            filter_ratio = (filtered_count / message_count * 100) if message_count > 0 else 0
            logger.info(f"Filter efficiency: {filter_ratio:.1f}% filtered out")
        
        # ìµœì¢… íŒŒí‹°ì…˜ë³„ í†µê³„
        if partition_stats:
            logger.info("=== Final Partition Statistics ===")
            total_messages = sum(partition_stats.values())
            for partition, count in sorted(partition_stats.items()):
                percentage = (count / total_messages * 100) if total_messages > 0 else 0
                logger.info(f"Partition {partition:2d}: {count:8d} messages ({percentage:5.1f}%)")
            logger.info(f"Total:        {total_messages:8d} messages")
            logger.info("==================================")
        
        # ëª¨ë“œë³„ ì˜¤í”„ì…‹ ì²˜ë¦¬
        if self.mode in ['incremental', 'continue'] and len(all_data) > 0:
            try:
                self.consumer.commit()
                logger.info(f"Committed offsets for {self.mode} mode")
                
                # ì»¤ë°‹ í›„ íŒŒí‹°ì…˜ ìƒíƒœ ë‹¤ì‹œ í™•ì¸
                if hasattr(self, '_log_partition_status'):
                    self._log_partition_status(self.consumer, list(self.consumer.assignment()))
                
            except Exception as e:
                logger.warning(f"Failed to commit offsets: {e}")
        
        return all_data
    
    def run(self):
        """ë°±í•„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ - ëª¨ë“œë³„ ë™ì‘"""
        try:
            logger.info(f"Starting backfill process in {self.mode} mode...")
            
            # 1. ëª¨ë“œë³„ ë°ì´í„° ìˆ˜ì§‘
            all_data = self.collect_all_data()
            
            if not all_data:
                logger.warning(f"No data collected in {self.mode} mode")
                if self.mode == 'incremental':
                    logger.info("This is normal for incremental mode if no new data is available")
                return
            
            # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬ ë° ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            logger.info("Processing timestamps and calculating date range...")
            processed_data, min_timestamp, max_timestamp = self.process_data_timestamps(all_data)
            
            if not processed_data:
                logger.warning("No valid data after timestamp processing")
                return
            
            # 3. íŒŒì¼ëª… ìƒì„± (ëª¨ë“  ëª¨ë“œì—ì„œ timestamp ê¸°ì¤€)
            filename = self.generate_filename(min_timestamp, max_timestamp)
            
            if self.mode == 'fresh':
                object_name = f"backfill/{filename}"
            elif self.mode == 'incremental':
                object_name = f"backfill/{filename}"
            elif self.mode == 'continue':
                object_name = f"backfill/{filename}"
            
            logger.info(f"Generated filename: {filename}")
            logger.info(f"Object name: {object_name}")
            
            # 4. DataFrame ìƒì„± ë° ì €ì¥
            logger.info("Creating DataFrame and saving to MinIO...")
            df = pd.DataFrame(processed_data)
            
            # ë°ì´í„° ì •ë³´ ë¡œê¹…
            logger.info(f"DataFrame created with {len(df)} records and {len(df.columns)} columns")
            if 'timestamp' in df.columns:
                logger.info(f"Timestamp range in data: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
            
            # MinIOì— ì €ì¥
            if self.save_parquet_to_minio(df, object_name):
                logger.info(f"Backfill process completed successfully in {self.mode} mode!")
                logger.info(f"Saved file: {object_name}")
                logger.info(f"Total records: {len(df)}")
                
                if min_timestamp and max_timestamp:
                    logger.info(f"Date range: {min_timestamp.strftime('%Y-%m-%d')} ~ {max_timestamp.strftime('%Y-%m-%d')} (KST)")
            else:
                logger.error("Failed to save data to MinIO")
            
        except Exception as e:
            logger.error(f"Error during backfill process: {e}")
            raise
        finally:
            # ì»¨ìŠˆë¨¸ ì¢…ë£Œ
            if hasattr(self, 'consumer'):
                self.consumer.close()
                logger.info("Kafka consumer closed")

class BackfillScheduler:
    def __init__(self):
        self.is_running = False
        self.last_run = None
        self.job_count = 0
        
        # ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        self.should_stop = False
        
    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬ (Ctrl+C, Docker stop ë“±)"""
        logger.info(f"Received signal {signum}, gracefully shutting down...")
        self.should_stop = True
        
    def run_incremental_job(self):
        """incremental ëª¨ë“œ ë°±í•„ ì‘ì—… ì‹¤í–‰"""
        if self.is_running:
            logger.warning("Previous job is still running, skipping this execution")
            return
            
        try:
            self.is_running = True
            self.job_count += 1
            start_time = datetime.now() + timedelta(hours=9) # KSTë¡œ ë³€í™˜
            
            logger.info("ğŸš€ " + "=" * 60)
            logger.info(f"Starting scheduled incremental job #{self.job_count}")
            logger.info(f"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info("=" * 64)
            
            # BackfillLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
            loader = BackfillLoader(mode='incremental')
            loader.run()
            duration_time = datetime.now() + timedelta(hours=9)  # KSTë¡œ ë³€í™˜
            duration = (duration_time - start_time).total_seconds()
            self.last_run = start_time
            
            logger.info("âœ… " + "=" * 60)
            logger.info(f"Scheduled incremental job #{self.job_count} completed successfully")
            logger.info(f"Duration: {duration:.1f} seconds")
            logger.info("=" * 64)
            
        except Exception as e:
            logger.error(f"âŒ Error in scheduled job #{self.job_count}: {e}")
            logger.exception("Full error traceback:")
        finally:
            self.is_running = False
    
    def get_next_run_time(self):
        """ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚° (3ì¼ í›„ ìì •)"""
        now = datetime.now() + timedelta(hours=9)  # KSTë¡œ ë³€í™˜
        # ë‚´ì¼ ìì •
        tomorrow_midnight = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        # 3ì¼ í›„ ìì • (ë‚´ì¼ë¶€í„° 2ì¼ í›„)
        next_run = tomorrow_midnight # test í•˜ë£¨ í›„ ì‹¤í–‰
        # next_run = tomorrow_midnight + timedelta(days=2)  # ì´ 3ì¼ í›„
        # next_run = now  + timedelta(hours=2) # í…ŒìŠ¤íŠ¸ìš© 2ì‹œê°„ í›„
        return next_run
    
    def start_scheduler(self):
        """ì •í™•í•œ ì‹œê°„ì— ì‹¤í–‰í•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬"""
        KST_NOW = datetime.now() + timedelta(hours=9)  # KSTë¡œ ë³€í™˜
        logger.info("ğŸ¯ Starting Precise Scheduler for Backfill Incremental")
        logger.info("Will run incremental job every 3 days at midnight (00:00)")
        logger.info(f"Scheduler started at: {KST_NOW.strftime('%Y-%m-%d %H:%M:%S')}")

        while not self.should_stop:
            try:
                # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                next_run_time = self.get_next_run_time()
                now = datetime.now() + timedelta(hours=9)  # KSTë¡œ ë³€í™˜
                
                # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
                sleep_seconds = (next_run_time - now).total_seconds()
                
                if sleep_seconds > 0:
                    sleep_duration = timedelta(seconds=int(sleep_seconds))
                    
                    logger.info("ğŸ’¤ Sleeping until next scheduled run...")
                    logger.info(f"   Current time: {now.strftime('%Y-%m-%d %H:%M:%S')}")
                    logger.info(f"   Next run: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    logger.info(f"   Sleep duration: {sleep_duration}")
                    logger.info("   Press Ctrl+C to stop the scheduler")
                    logger.info("-" * 50)
                    
                    # ì¸í„°ëŸ½íŠ¸ ê°€ëŠ¥í•œ sleep (í° ì‹œê°„ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²´í¬)
                    remaining_seconds = sleep_seconds
                    while remaining_seconds > 0 and not self.should_stop:
                        # ìµœëŒ€ 1ì‹œê°„ì”© ì ë“¤ê³  ì¤‘ê°„ì— ì²´í¬
                        sleep_chunk = min(remaining_seconds, 3600)  # 1ì‹œê°„ ë˜ëŠ” ë‚¨ì€ ì‹œê°„
                        time.sleep(sleep_chunk)
                        remaining_seconds -= sleep_chunk
                        
                        # 24ì‹œê°„ë§ˆë‹¤ ì¤‘ê°„ ìƒíƒœ ë¡œê·¸ (ì„ íƒì‚¬í•­)
                        if remaining_seconds > 0 and int(remaining_seconds) % (24 * 3600) == 0:
                            days_left = int(remaining_seconds / (24 * 3600))
                            logger.info(f"â° Still sleeping... {days_left} days left until next run")
                
                # ì¢…ë£Œ ì‹ í˜¸ê°€ ì™”ìœ¼ë©´ ë£¨í”„ íƒˆì¶œ
                if self.should_stop:
                    break
                
                # ì‹œê°„ì´ ë˜ë©´ ì‘ì—… ì‹¤í–‰
                logger.info(f"â° Scheduled time reached! Current time: {KST_NOW.strftime('%Y-%m-%d %H:%M:%S')}")
                self.run_incremental_job()
                
            except KeyboardInterrupt:
                logger.info("ğŸ‘‹ Scheduler interrupted by user")
                break
            except Exception as e:
                logger.error(f"âŒ Scheduler error: {e}")
                logger.info("â³ Waiting 5 minutes before retry...")
                
                # 5ë¶„ ëŒ€ê¸° (ì¸í„°ëŸ½íŠ¸ ê°€ëŠ¥)
                for _ in range(300):  # 5ë¶„ = 300ì´ˆ
                    if self.should_stop:
                        break
                    time.sleep(1)
        
        logger.info("ğŸ”š Precise Scheduler stopped")

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='Backfill Loader - Kafka to MinIO data loader')
    
    parser.add_argument('--mode', 
                        choices=['fresh', 'incremental', 'continue', 'scheduler'], 
                        default='fresh',
                        help='Execution mode: fresh (start from beginning), incremental (from last offset), continue (from last offset or beginning), scheduler (run as a scheduled backfill job for every 3days)')

    parser.add_argument('--from-date',
                        type=str,
                        help='Start date for continue mode (YYYY-MM-DD format)')
    
    parser.add_argument('--to-date',
                        type=str,
                        help='End date (YYYY-MM-DD format, inclusive)')
    
    parser.add_argument('--target-date',
                        type=str,
                        help='Specific single date (YYYY-MM-DD format)')
    
    parser.add_argument('--kafka-brokers',
                        type=str,
                        help='Kafka broker addresses (comma-separated)')
    
    parser.add_argument('--kafka-topic',
                        type=str,
                        help='Kafka topic name')
    
    parser.add_argument('--kafka-group-id',
                        type=str,
                        help='Kafka consumer group ID')
    
    parser.add_argument('--minio-endpoint',
                        type=str,
                        help='MinIO endpoint')
    
    parser.add_argument('--minio-bucket',
                        type=str,
                        help='MinIO bucket name')
    
    return parser.parse_args()

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    args = parse_arguments()
    
    # í™˜ê²½ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ
    if args.kafka_brokers:
        os.environ['KAFKA_BROKERS'] = args.kafka_brokers
    if args.kafka_topic:
        os.environ['KAFKA_TOPIC'] = args.kafka_topic
    if args.kafka_group_id:
        os.environ['KAFKA_GROUP_ID'] = args.kafka_group_id
    if args.minio_endpoint:
        os.environ['MINIO_ENDPOINT'] = args.minio_endpoint
    if args.minio_bucket:
        os.environ['MINIO_BUCKET'] = args.minio_bucket
    
    # ëª¨ë“œ ê²€ì¦
    if args.target_date and (args.from_date or args.to_date):
        logger.error("Cannot use --target-date with --from-date or --to-date")
        sys.exit(1)
    
    date_args = [args.from_date, args.to_date, args.target_date]
    for date_str in filter(None, date_args):
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
        except ValueError:
            logger.error(f"Invalid date format: {date_str}. Use YYYY-MM-DD")
            sys.exit(1)

    if args.mode == 'continue' and args.from_date:
        try:
            datetime.strptime(args.from_date, '%Y-%m-%d')
        except ValueError:
            logger.error("Invalid from-date format. Use YYYY-MM-DD")
            sys.exit(1)

    if args.mode == 'scheduler':
        # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
        logger.info("Starting in scheduler mode - will run incremental mode every 3 days")
        scheduler = BackfillScheduler()
        scheduler.start_scheduler()

    # ë°±í•„ ë¡œë” ì‹¤í–‰
    logger.info(f"Starting Backfill Loader with arguments: "
               f"mode={args.mode}, from_date={args.from_date}, "
               f"to_date={args.to_date}, target_date={args.target_date}")
    
    loader = BackfillLoader(
        mode=args.mode, 
        from_date=args.from_date,
        to_date=args.to_date,
        target_date=args.target_date
    )
    loader.run()